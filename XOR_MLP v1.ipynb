{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/11/2019\n",
    "\n",
    "The code here is close to Nielsen. Each activation is treated as a column vector, even the last one which for XOR is just a simple number and is encloded in a shape (1,1) column vector of just one row, i.e if activation value of output neuron is a, then it is computed as np.array([[a]]).\n",
    "\n",
    "Can easily adapt code here for the MLP excercises and the Iris classification problem.\n",
    "But you may need to use more than 2 hidden neurons and more than 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return  1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a*(1 - a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(2,2)\n",
    "        self.b2 = np.random.randn(2,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,2)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = XOR_MLP()\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 3.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one.\n",
    "\n",
    "class XOR_MLP2:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 3 neurons\n",
    "        self.w2 = np.random.randn(3,2)\n",
    "        self.b2 = np.random.randn(3,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,3)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "    \n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):\n",
    "            a1 = x.reshape(2,1)\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)\n",
    "            \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "            \n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y)**2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "    \n",
    "xor2 = XOR_MLP2()\n",
    "xs = xor2.train_inputs.T\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "c = xor2.train(epochs, 3.0)\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, m, n, o):\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        # Hidden layer weights and biases\n",
    "        self.w2 = np.random.randn(n, m)\n",
    "        self.b2 = np.random.randn(n, 1)\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        self.w3 = np.random.randn(o, n)\n",
    "        self.b3 = np.random.randn(o, 1)\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "\n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            # Reshape x and y to be column vectors\n",
    "            a1 = x.reshape(self.m, 1)\n",
    "            y = y.reshape(self.o, 1)\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            a2 = a2.reshape(self.n, 1)\n",
    "            \n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "\n",
    "            a3 = a3.reshape(self.o, 1)\n",
    "            \n",
    "            delta3 = (a3 - y) * sigm_deriv(z3)\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y) ** 2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        \n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "\n",
    "    def human_readable_feedforward(self, xs):\n",
    "        a3s = self.feedforward(xs)\n",
    "        for x, a3 in zip(xs.T, a3s.T):\n",
    "            print(f'Input: {x} Output: {a3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP but uses cross entropy cost function\n",
    "\n",
    "class MLP_CrossEntropy:\n",
    "    def __init__(self, m, n, o):\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        # Hidden layer weights and biases\n",
    "        self.w2 = np.random.randn(n, m)\n",
    "        self.b2 = np.random.randn(n, 1)\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        self.w3 = np.random.randn(o, n)\n",
    "        self.b3 = np.random.randn(o, 1)\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "\n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            # Reshape x and y to be column vectors\n",
    "            a1 = x.reshape(self.m, 1)\n",
    "            y = y.reshape(self.o, 1)\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            a2 = a2.reshape(self.n, 1)\n",
    "            \n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "\n",
    "            a3 = a3.reshape(self.o, 1)\n",
    "            \n",
    "            # Cross entropy cost function \n",
    "            delta3 = (a3 - y)\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y) ** 2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        \n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "\n",
    "    def human_readable_feedforward(self, xs):\n",
    "        a3s = self.feedforward(xs)\n",
    "        for x, a3 in zip(xs.T, a3s.T):\n",
    "            print(f'Input: {x} Output: {a3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use MLP to learn XOR\n",
    "\n",
    "xor = MLP(2,3,1)\n",
    "\n",
    "xor.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "xor.train_outputs = np.array([0,1,1,0])\n",
    "\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 9.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_mlp = MLP(3,3,1)\n",
    "\n",
    "and_mlp.train_inputs = np.array([[0,0,0], [0,0,1], [0,1,0], [0,1,1], [1,0,0],[1,0,1],[1,1,0],[1,1,1]])\n",
    "and_mlp.train_outputs = np.array([0,0,0,0,0,0,0,1])\n",
    "\n",
    "xs = and_mlp.train_inputs.T\n",
    "\n",
    "print(and_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "c = and_mlp.train(epochs, 3.0)\n",
    "\n",
    "print(and_mlp.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 \n",
    "\n",
    "p1_mlp = MLP(3,4,1)\n",
    "\n",
    "p1_mlp.train_inputs = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "p1_mlp.train_outputs = np.array([0,1,1,0])\n",
    "\n",
    "xs = p1_mlp.train_inputs.T\n",
    "\n",
    "print(p1_mlp.feedforward(xs))\n",
    "\n",
    "# 2000 Iterations\n",
    "epochs = 2000\n",
    "learning_rate = 9.0\n",
    "\n",
    "#For 2000 iternations a learning rate of above 9 or 10 will cause divergence\n",
    "\n",
    "#Output after 1500 epochs with learning of 9 [[0.00968279 0.97926286 0.98020147 0.02596714]]\n",
    "#For 1500 and 2000 epochs the 4th element is still 0.025 while the 1st element is 0.009/0.008\n",
    "#Both are close to 0 but its curious how the first element is so much smaller than the 4th\n",
    "#The 2nd and 3rd elements are both close close to 1 and similiar in value\n",
    "\n",
    "c = p1_mlp.train(epochs, learning_rate)\n",
    "\n",
    "print(p1_mlp.feedforward(xs))\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 Neural Network with 3 input and 2 output neurons\n",
    "\n",
    "p2_mlp = MLP(3,1,2)\n",
    "\n",
    "p2_mlp.train_inputs = np.array([[1,1,0], [1,-1,-1], [-1,1,1], [-1,-1,1], [0,1,-1], [0,-1,-1], [1,1,1]])\n",
    "\n",
    "p2_mlp.train_outputs = np.array([[1,0], [0,1], [1,1], [1,0], [1,0], [1,1], [1,1]])\n",
    "\n",
    "xs = p2_mlp.train_inputs.T\n",
    "\n",
    "\n",
    "print(p2_mlp.feedforward(xs))\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "c = p2_mlp.train(epochs, 17.0)\n",
    "\n",
    "print(p2_mlp.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 Transportation Mode Choice\n",
    "\n",
    "# Gender (Male)0 or (Female)1\n",
    "# Car ownership 0 , 1 or 2\n",
    "# Travel cost (Cheap) 0 , (Standard) 1 or (Expensive) 2\n",
    "# Income (Low) 0, (Medium) 1 or (High) 2\n",
    "\n",
    "# Output (Bus) [1,0,0] , (Train) [0,1,0] or (Car) [0,0,1]\n",
    "\n",
    "transport_mlp = MLP(4,25,3)\n",
    "\n",
    "transport_mlp.train_inputs = np.array([[0,0,0,0], [0,1,0,1], [1,1,0,1], [1,0,0,0], [0,1,0,1], [0,0,1,1], [1,1,1,1], [1,1,2,2], [0,2,2,1], [1,2,2,2]])\n",
    "transport_mlp.train_outputs = np.array([[1,0,0], [1,0,0], [0,1,0], [1,0,0], [1,0,0], [0,1,0], [0,1,0], [0,0,1], [0,0,1], [0,0,1]])\n",
    "\n",
    "# Initial output\n",
    "xs = transport_mlp.train_inputs.T\n",
    "print(transport_mlp.feedforward(xs))\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "c = transport_mlp.train(epochs, 3.0)\n",
    "\n",
    "print(transport_mlp.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "#Woman, no car, expensive, medium income\n",
    "xs = np.array([1,0,2,1]).reshape(4,1)\n",
    "print(transport_mlp.predict(xs))\n",
    "\n",
    "\n",
    "#Use pandas to copy training inputs into a dataframe and save as csv seperated by commas\n",
    "df = pd.DataFrame(transport_mlp.train_inputs)\n",
    "df.to_csv('transport.csv', index=False, header=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 Iris Dataset\n",
    "#Import iris_data.csv into a dataframe\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Last column is the output\n",
    "training_outputs = df.iloc[:,-1]\n",
    "\n",
    "training_outputs = pd.get_dummies(training_outputs)\n",
    "\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "\n",
    "#Convert y true and false to 1 and 0\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "\n",
    "#Drop last column from dataframe\n",
    "df = df.drop(df.columns[[-1]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "\n",
    "iris_mlp = MLP(4,25,3)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 1000\n",
    "c = iris_mlp.train(epochs, 0.4)\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iris_mlp = MLP_CrossEntropy(4,25,3)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "c = iris_mlp.train(epochs, 0.15)\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "#Initial impressions\n",
    "#Used 1.0 learning constant same for mean squarred error\n",
    "# Cross Entropy was 1 second faster than MSE\n",
    "# Converged faster than MSE\n",
    "# Diverages easier than MSE\n",
    "# MSE is more stable than Cross Entropy\n",
    "# Needs a lower learning constant than MSE\n",
    "\n",
    "#A learning constant of > 0.2 causes divergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
