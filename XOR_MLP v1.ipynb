{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22/11/2019\n",
    "\n",
    "The code here is close to Nielsen. Each activation is treated as a column vector, even the last one which for XOR is just a simple number and is encloded in a shape (1,1) column vector of just one row, i.e if activation value of output neuron is a, then it is computed as np.array([[a]]).\n",
    "\n",
    "Can easily adapt code here for the MLP excercises and the Iris classification problem.\n",
    "But you may need to use more than 2 hidden neurons and more than 1 output neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.special import expit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return  1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a*(1 - a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigm(z):\n",
    "    return expit(z)\n",
    "\n",
    "def sigm_deriv(z):\n",
    "    a = sigm(z)\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR_MLP:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 2 neurons\n",
    "        self.w2 = np.random.randn(2,2)\n",
    "        self.b2 = np.random.randn(2,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,2)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "        \n",
    "\n",
    "    def feedforward(self, xs):    \n",
    "        # here xs is a matrix where each column is an input vector\n",
    "        # w2.dot(xs) applies the weight matrix w2 to each input at once\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)            \n",
    "        return a3s\n",
    "\n",
    "    \n",
    "    def backprop(self, xs, ys):   # Assumed here that input vectors are rows in xs\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):               # for zip to work, each x in xs must be a row vector\n",
    "            a1 = x.reshape(2,1)              # convert input row vector x into (2,1) column vector\n",
    "            z2 = self.w2.dot(a1) + self.b2   # so will z2 and a2\n",
    "            a2 = sigm(z2)                    # column vector shape (2,1)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3   # a simple number in a (1,1) column vector\n",
    "            a3 = sigm(z3)                    # so is a3\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)                   # delta3.shape is (1,1)\n",
    "           \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))  # w3 shape is (1,2), w3.T shape is (2,1)\n",
    "                                                               # delta2 is shape (2,1)\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)  # shape (1,1) by (1,2) gives (1,2)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)  # shape (2,1) by (1,2) gives (2,2)\n",
    "        \n",
    "           \n",
    "            cost += ((a3 - y)**2).sum() \n",
    "        \n",
    "        n = len(ys)  # number of training vectors    \n",
    "        \n",
    "        # get the average change per training input  \n",
    "        # return the average adjustments to the biases and weights \n",
    "        # in each layer and the cost\n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "        \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "                \n",
    "                \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor = XOR_MLP()\n",
    "xs = xor.train_inputs.T\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "c = xor.train(epochs, 3.0)\n",
    "\n",
    "print(xor.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 1: copy and adapt the above XOR_MLP code so that it uses 3 neurons in the hidden layer. Train such a MLP and see if it learns faster than the previous one.\n",
    "\n",
    "class XOR_MLP2:\n",
    "    def __init__(self):\n",
    "        self.train_inputs = np.array([[0,0], [0,1], [1,0], [1,1]])\n",
    "        self.train_outputs = np.array([0,1,1,0])\n",
    "          \n",
    "        np.random.seed(23)\n",
    "        # hidden layer of 3 neurons\n",
    "        self.w2 = np.random.randn(3,2)\n",
    "        self.b2 = np.random.randn(3,1)\n",
    "        \n",
    "        # output layer has 1 neuron\n",
    "        self.w3 = np.random.randn(1,3)\n",
    "        self.b3 = np.random.randn(1,1)\n",
    "    \n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        \n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x,y in zip(xs,ys):\n",
    "            a1 = x.reshape(2,1)\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "            \n",
    "            delta3 = (a3-y) * sigm_deriv(z3)\n",
    "            \n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "            \n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y)**2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2/n, del_w2/n, del_b3/n, del_w3/n, cost/n\n",
    "    \n",
    "    def train(self, epochs, eta):\n",
    "        xs = self.train_inputs\n",
    "        ys = self.train_outputs\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2,d_w2,d_b3,d_w3, cost[e] = self.backprop(xs,ys)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        plt.plot(cost)\n",
    "        return cost\n",
    "    \n",
    "xor2 = XOR_MLP2()\n",
    "xs = xor2.train_inputs.T\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "c = xor2.train(epochs, 3.0)\n",
    "\n",
    "print(xor2.feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "fig, axs = plt.subplots(3,1,figsize=(10,15))\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(x_axis, c)\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(x_axis[:61], c[:61])\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(x_axis[900:], c[900:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    # m is the number of inputs, n is the number of neurons in the hidden layer, o is the number of outputs\n",
    "    def __init__(self, m, n, o):\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        # Hidden layer weights and biases\n",
    "        self.w2 = np.random.randn(n, m)\n",
    "        self.b2 = np.random.randn(n, 1)\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        self.w3 = np.random.randn(o, n)\n",
    "        self.b3 = np.random.randn(o, 1)\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "\n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            # Reshape x and y to be column vectors\n",
    "            a1 = x.reshape(self.m, 1)\n",
    "            y = y.reshape(self.o, 1)\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            a2 = a2.reshape(self.n, 1)\n",
    "            \n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "\n",
    "            a3 = a3.reshape(self.o, 1)\n",
    "            \n",
    "            delta3 = (a3 - y) * sigm_deriv(z3)\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y) ** 2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        \n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "\n",
    "    def human_readable_feedforward(self, xs):\n",
    "        a3s = self.feedforward(xs)\n",
    "        for x, a3 in zip(xs.T, a3s.T):\n",
    "            print(f'Input: {x} Output: {a3}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLP but uses cross entropy cost function\n",
    "\n",
    "class MLP_CrossEntropy:\n",
    "    def __init__(self, m, n, o):\n",
    "        np.random.seed(23)\n",
    "        \n",
    "        # Hidden layer weights and biases\n",
    "        self.w2 = np.random.randn(n, m)\n",
    "        self.b2 = np.random.randn(n, 1)\n",
    "        \n",
    "        # Output layer weights and biases\n",
    "        self.w3 = np.random.randn(o, n)\n",
    "        self.b3 = np.random.randn(o, 1)\n",
    "\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.o = o\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        a2s = sigm(self.w2.dot(xs) + self.b2)\n",
    "        a3s = sigm(self.w3.dot(a2s) + self.b3)\n",
    "        return a3s\n",
    "\n",
    "    def backprop(self, xs, ys):\n",
    "        del_w2 = np.zeros(self.w2.shape, dtype=float)\n",
    "        del_b2 = np.zeros(self.b2.shape, dtype=float)\n",
    "        del_w3 = np.zeros(self.w3.shape, dtype=float)\n",
    "        del_b3 = np.zeros(self.b3.shape, dtype=float)\n",
    "        cost = 0.0\n",
    "        \n",
    "        for x, y in zip(xs, ys):\n",
    "            # Reshape x and y to be column vectors\n",
    "            a1 = x.reshape(self.m, 1)\n",
    "            y = y.reshape(self.o, 1)\n",
    "\n",
    "            z2 = self.w2.dot(a1) + self.b2\n",
    "            a2 = sigm(z2)\n",
    "            a2 = a2.reshape(self.n, 1)\n",
    "            \n",
    "            \n",
    "            z3 = self.w3.dot(a2) + self.b3\n",
    "            a3 = sigm(z3)\n",
    "\n",
    "            a3 = a3.reshape(self.o, 1)\n",
    "            \n",
    "            # Cross entropy cost function \n",
    "            delta3 = (a3 - y)\n",
    "            delta2 = sigm_deriv(z2) * (self.w3.T.dot(delta3))\n",
    "\n",
    "            del_b3 += delta3\n",
    "            del_w3 += delta3.dot(a2.T)\n",
    "            \n",
    "            del_b2 += delta2\n",
    "            del_w2 += delta2.dot(a1.T)\n",
    "            \n",
    "            cost += ((a3 - y) ** 2).sum()\n",
    "        \n",
    "        n = len(ys)\n",
    "        \n",
    "        return del_b2 / n, del_w2 / n, del_b3 / n, del_w3 / n, cost / n\n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            d_b2, d_w2, d_b3, d_w3, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "                \n",
    "            self.b2 -= eta * d_b2\n",
    "            self.w2 -= eta * d_w2\n",
    "            self.b3 -= eta * d_b3\n",
    "            self.w3 -= eta * d_w3\n",
    "        \n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "\n",
    "    def human_readable_feedforward(self, xs):\n",
    "        a3s = self.feedforward(xs)\n",
    "        for x, a3 in zip(xs.T, a3s.T):\n",
    "            print(f'Input: {x} Output: {a3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_MLP:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        np.random.seed(23)\n",
    "\n",
    "        #Get number of layers and sizes of each layer\n",
    "        self.num_layers = len(hidden_layer_sizes) + 1\n",
    "        self.layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "\n",
    "        #Initialize weights and biases\n",
    "        self.weights = [np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) for i in range(self.num_layers)]\n",
    "        self.biases = [np.random.randn(self.layer_sizes[i+1], 1) for i in range(self.num_layers)]\n",
    "\n",
    "        #Print weights and biases shape\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            print(w.shape, b.shape)\n",
    "\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        actual_outputs = [xs]\n",
    "        for i in range(self.num_layers):\n",
    "            #Weighted sum and activation function\n",
    "            #Actual output -1 because the only actual output is the last layer\n",
    "            a = sigm(self.weights[i].dot(actual_outputs[-1]) + self.biases[i])\n",
    "            actual_outputs.append(a)\n",
    "\n",
    "        return actual_outputs[-1]\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_ws = [np.zeros(w.shape, dtype=float) for w in self.weights]\n",
    "        del_bs = [np.zeros(b.shape, dtype=float) for b in self.biases]\n",
    "        cost = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            x = x.reshape(self.layer_sizes[0], 1)\n",
    "            y = y.reshape(self.layer_sizes[-1], 1)\n",
    "\n",
    "            actual_outputs = [x]\n",
    "            zs = []\n",
    "            #Feedforward\n",
    "            for i in range(self.num_layers):\n",
    "                z = self.weights[i].dot(actual_outputs[-1]) + self.biases[i]\n",
    "                zs.append(z)\n",
    "                a = sigm(z)\n",
    "                actual_outputs.append(a)\n",
    "            \n",
    "            cost += ((actual_outputs[-1] - y) ** 2).sum()\n",
    "\n",
    "            #Initialize list for deltas\n",
    "            deltas = [None] * (self.num_layers)\n",
    "            #Delta for output layer\n",
    "            deltas[-1] = (actual_outputs[-1] - y) * sigm_deriv(zs[-1])\n",
    "\n",
    "            #Delta for hidden layers\n",
    "            for i in reversed(range(self.num_layers-1)):\n",
    "                deltas[i] = sigm_deriv(zs[i]) * (self.weights[i+1].T.dot(deltas[i+1]))\n",
    "\n",
    "            #Adjust weights and biases\n",
    "            for i in range(self.num_layers):\n",
    "                del_bs[i] += deltas[i]\n",
    "                del_ws[i] += deltas[i].dot(actual_outputs[i].T)\n",
    "\n",
    "        n = len(ys)\n",
    "        #Return average adjustments to weights and biases and cost\n",
    "        return [del_b / n for del_b in del_bs], [del_w / n for del_w in del_ws], cost / n\n",
    "        \n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            d_bs, d_ws, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                self.biases[i] += -eta * d_bs[i]\n",
    "                self.weights[i] += -eta * d_ws[i]\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "    \n",
    "    def human_readable_feedforward(self, xs):\n",
    "        outputs = self.feedforward(xs)\n",
    "        for x, y in zip(xs.T, outputs.T):\n",
    "            print(f'Input: {x} Output: {y}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generic_MLP_CrossEntropy:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        np.random.seed(23)\n",
    "\n",
    "        #Get number of layers and sizes of each layer\n",
    "        self.num_layers = len(hidden_layer_sizes) + 1\n",
    "        self.layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "\n",
    "        #Initialize weights and biases\n",
    "        self.weights = [np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) for i in range(self.num_layers)]\n",
    "        self.biases = [np.random.randn(self.layer_sizes[i+1], 1) for i in range(self.num_layers)]\n",
    "\n",
    "        #Print weights and biases shape\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            print(w.shape, b.shape)\n",
    "\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        actual_outputs = [xs]\n",
    "        for i in range(self.num_layers):\n",
    "            #Weighted sum and activation function\n",
    "            #Actual output -1 because the only actual output is the last layer\n",
    "            a = sigm(self.weights[i].dot(actual_outputs[-1]) + self.biases[i])\n",
    "            actual_outputs.append(a)\n",
    "\n",
    "        return actual_outputs[-1]\n",
    "    \n",
    "    def backprop(self, xs, ys):\n",
    "        del_ws = [np.zeros(w.shape, dtype=float) for w in self.weights]\n",
    "        del_bs = [np.zeros(b.shape, dtype=float) for b in self.biases]\n",
    "        cost = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            x = x.reshape(self.layer_sizes[0], 1)\n",
    "            y = y.reshape(self.layer_sizes[-1], 1)\n",
    "\n",
    "            actual_outputs = [x]\n",
    "            zs = []\n",
    "            #Feedforward\n",
    "            for i in range(self.num_layers):\n",
    "                z = self.weights[i].dot(actual_outputs[-1]) + self.biases[i]\n",
    "                zs.append(z)\n",
    "                a = sigm(z)\n",
    "                actual_outputs.append(a)\n",
    "            \n",
    "            cost += ((actual_outputs[-1] - y) ** 2).sum()\n",
    "\n",
    "            #Initialize list for deltas\n",
    "            deltas = [None] * (self.num_layers)\n",
    "            #Delta for output layer\n",
    "            #Cross entropy cost function\n",
    "            deltas[-1] = (actual_outputs[-1] - y)\n",
    "\n",
    "            #Delta for hidden layers\n",
    "            for i in reversed(range(self.num_layers - 1)):\n",
    "                deltas[i] = sigm_deriv(zs[i]) * (self.weights[i+1].T.dot(deltas[i+1]))\n",
    "\n",
    "            #Adjust weights and biases\n",
    "            for i in range(self.num_layers):\n",
    "                del_bs[i] += deltas[i]\n",
    "                del_ws[i] += deltas[i].dot(actual_outputs[i].T)\n",
    "\n",
    "        n = len(ys)\n",
    "        #Return average adjustments to weights and biases and cost\n",
    "        return [del_b / n for del_b in del_bs], [del_w / n for del_w in del_ws], cost / n\n",
    "        \n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            d_bs, d_ws, cost[e] = self.backprop(self.train_inputs, self.train_outputs)\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                self.biases[i] += -eta * d_bs[i]\n",
    "                self.weights[i] += -eta * d_ws[i]\n",
    "\n",
    "        plt.plot(cost)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return cost\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "    \n",
    "    def human_readable_feedforward(self, xs):\n",
    "        outputs = self.feedforward(xs)\n",
    "        for x, y in zip(xs.T, outputs.T):\n",
    "            print(f'Input: {x} Output: {y}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class validation_MLP:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size):\n",
    "        np.random.seed(23)\n",
    "\n",
    "        #Get number of layers and sizes of each layer\n",
    "        self.num_layers = len(hidden_layer_sizes) + 1\n",
    "        self.layer_sizes = [input_size] + hidden_layer_sizes + [output_size]\n",
    "\n",
    "        #Initialize weights and biases\n",
    "        self.weights = [np.random.randn(self.layer_sizes[i+1], self.layer_sizes[i]) for i in range(self.num_layers)]\n",
    "        self.biases = [np.random.randn(self.layer_sizes[i+1], 1) for i in range(self.num_layers)]\n",
    "\n",
    "        #Print weights and biases shape\n",
    "        for w, b in zip(self.weights, self.biases):\n",
    "            print(w.shape, b.shape)\n",
    "\n",
    "\n",
    "    def feedforward(self, xs):\n",
    "        actual_outputs = [xs]\n",
    "        for i in range(self.num_layers):\n",
    "            #Weighted sum and activation function\n",
    "            #Actual output -1 because the only actual output is the last layer\n",
    "            a = sigm(self.weights[i].dot(actual_outputs[-1]) + self.biases[i])\n",
    "            actual_outputs.append(a)\n",
    "\n",
    "        return actual_outputs[-1]\n",
    "    \n",
    "    def backprop(self, xs, ys, validation_xs=None, validation_ys=None):\n",
    "        del_ws = [np.zeros(w.shape, dtype=float) for w in self.weights]\n",
    "        del_bs = [np.zeros(b.shape, dtype=float) for b in self.biases]\n",
    "        cost = 0.0\n",
    "        validation_cost = 0.0\n",
    "\n",
    "        for x, y in zip(xs, ys):\n",
    "            x = x.reshape(self.layer_sizes[0], 1)\n",
    "            y = y.reshape(self.layer_sizes[-1], 1)\n",
    "\n",
    "            actual_outputs = [x]\n",
    "            zs = []\n",
    "            #Feedforward\n",
    "            for i in range(self.num_layers):\n",
    "                z = self.weights[i].dot(actual_outputs[-1]) + self.biases[i]\n",
    "                zs.append(z)\n",
    "                a = sigm(z)\n",
    "                actual_outputs.append(a)\n",
    "            \n",
    "            cost += ((actual_outputs[-1] - y) ** 2).sum()\n",
    "\n",
    "            #Initialize list for deltas\n",
    "            deltas = [None] * (self.num_layers)\n",
    "            #Delta for output layer\n",
    "            deltas[-1] = (actual_outputs[-1] - y) * sigm_deriv(zs[-1])\n",
    "\n",
    "            #Delta for hidden layers\n",
    "            for i in reversed(range(self.num_layers - 1)):\n",
    "                deltas[i] = sigm_deriv(zs[i]) * (self.weights[i+1].T.dot(deltas[i+1]))\n",
    "\n",
    "            #Adjust weights and biases\n",
    "            for i in range(self.num_layers):\n",
    "                del_bs[i] += deltas[i]\n",
    "                del_ws[i] += deltas[i].dot(actual_outputs[i].T)\n",
    "\n",
    "        if validation_xs is not None and validation_ys is not None:\n",
    "            # Compute validation cost\n",
    "            for val_x, val_y in zip(validation_xs, validation_ys):\n",
    "                val_x = val_x.reshape(self.layer_sizes[0], 1)\n",
    "                val_y = val_y.reshape(self.layer_sizes[-1], 1)\n",
    "\n",
    "                val_as = [val_x]\n",
    "                val_zs = []\n",
    "\n",
    "                for i in range(self.num_layers):\n",
    "                    val_z = self.weights[i].dot(val_as[-1]) + self.biases[i]\n",
    "                    val_zs.append(val_z)\n",
    "                    val_a = sigm(val_z)\n",
    "                    val_as.append(val_a)\n",
    "\n",
    "                validation_cost += ((val_as[-1] - val_y) ** 2).sum()\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        n = len(ys)\n",
    "        #Return average adjustments to weights and biases and cost\n",
    "        return [del_b / n for del_b in del_bs], [del_w / n for del_w in del_ws], cost / n, validation_cost / len(validation_ys)\n",
    "\n",
    "        \n",
    "\n",
    "    def train(self, epochs, eta):\n",
    "        cost = np.zeros((epochs,))\n",
    "        validation_cost = np.zeros((epochs,))\n",
    "\n",
    "        for e in range(epochs):\n",
    "            d_bs, d_ws, cost[e], validation_cost[e] = self.backprop(self.train_inputs, self.train_outputs, self.validation_inputs, self.validation_outputs)\n",
    "\n",
    "\n",
    "            for i in range(self.num_layers):\n",
    "                self.biases[i] -= eta * d_bs[i]\n",
    "                self.weights[i] -= eta * d_ws[i]\n",
    "        plt.plot(validation_cost, label='Validation Cost')\n",
    "        plt.plot(cost, label='Training Cost')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        return cost, validation_cost\n",
    "    \n",
    "    def predict(self, xs):\n",
    "        return self.feedforward(xs)\n",
    "    \n",
    "    def human_readable_feedforward(self, xs):\n",
    "        outputs = self.feedforward(xs)\n",
    "        for x, y in zip(xs.T, outputs.T):\n",
    "            print(f'Input: {x} Output: {y}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1 \n",
    "\n",
    "p1_mlp = MLP(3,4,1)\n",
    "\n",
    "p1_mlp.train_inputs = np.array([[0,0,1], [0,1,1], [1,0,1], [1,1,1]])\n",
    "p1_mlp.train_outputs = np.array([0,1,1,0])\n",
    "\n",
    "xs = p1_mlp.train_inputs.T\n",
    "\n",
    "print(p1_mlp.feedforward(xs))\n",
    "\n",
    "# 2000 Iterations\n",
    "epochs = 2000\n",
    "learning_rate = 11.0\n",
    "\n",
    "#For 2000 iternations a learning rate of above 9 or 10 will cause divergence\n",
    "\n",
    "#Output after 1500 epochs with learning of 9 [[0.00968279 0.97926286 0.98020147 0.02596714]]\n",
    "#For 1500 and 2000 epochs the 4th element is still 0.025 while the 1st element is 0.009/0.008\n",
    "#Both are close to 0 but its curious how the first element is so much smaller than the 4th\n",
    "#The 2nd and 3rd elements are both close close to 1 and similiar in value\n",
    "\n",
    "c = p1_mlp.train(epochs, learning_rate)\n",
    "\n",
    "print(p1_mlp.feedforward(xs))\n",
    "\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 2 Neural Network with 3 input and 2 output neurons\n",
    "\n",
    "p2_mlp = MLP(3,4,2)\n",
    "\n",
    "p2_mlp.train_inputs = np.array([[1,1,0], [1,-1,-1], [-1,1,1], [-1,-1,1], [0,1,-1], [0,-1,-1], [1,1,1]])\n",
    "\n",
    "p2_mlp.train_outputs = np.array([[1,0], [0,1], [1,1], [1,0], [1,0], [1,1], [1,1]])\n",
    "\n",
    "xs = p2_mlp.train_inputs.T\n",
    "\n",
    "\n",
    "epochs = 2000\n",
    "\n",
    "c = p2_mlp.train(epochs, 15.0)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "print(p2_mlp.human_readable_feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 3 Transportation Mode Choice\n",
    "\n",
    "# Gender (Male)0 or (Female)1\n",
    "# Car ownership 0 , 1 or 2\n",
    "# Travel cost (Cheap) 0 , (Standard) 1 or (Expensive) 2\n",
    "# Income (Low) 0, (Medium) 1 or (High) 2\n",
    "\n",
    "# Output (Bus) [1,0,0] , (Train) [0,1,0] or (Car) [0,0,1]\n",
    "\n",
    "transport_mlp = MLP(4,6,3)\n",
    "\n",
    "transport_mlp.train_inputs = np.array([[0,0,0,0], [0,1,0,1], [1,1,0,1], [1,0,0,0], [0,1,0,1], [0,0,1,1], [1,1,1,1], [1,1,2,2], [0,2,2,1], [1,2,2,2]])\n",
    "transport_mlp.train_outputs = np.array([[1,0,0], [1,0,0], [0,1,0], [1,0,0], [1,0,0], [0,1,0], [0,1,0], [0,0,1], [0,0,1], [0,0,1]])\n",
    "\n",
    "print(transport_mlp.train_inputs.shape)\n",
    "print(transport_mlp.train_outputs.shape)\n",
    "\n",
    "# Initial output\n",
    "xs = transport_mlp.train_inputs.T\n",
    "print(transport_mlp.feedforward(xs))\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "c = transport_mlp.train(epochs, 8.0)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "print(transport_mlp.human_readable_feedforward(xs))\n",
    "\n",
    "x_axis = np.linspace(1, epochs, epochs, dtype=int)\n",
    "\n",
    "#Woman, no car, expensive, medium income\n",
    "xs = np.array([1,0,2,1]).reshape(4,1)\n",
    "print(transport_mlp.predict(xs))\n",
    "\n",
    "\n",
    "#Use pandas to copy training inputs into a dataframe and save as csv seperated by commas\n",
    "df = pd.DataFrame(transport_mlp.train_inputs)\n",
    "df.to_csv('transport.csv', index=False, header=False, sep=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 Iris Dataset\n",
    "#Import iris_data.csv into a dataframe\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Last column is the output\n",
    "training_outputs = df.iloc[:,-1]\n",
    "\n",
    "training_outputs = pd.get_dummies(training_outputs)\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "\n",
    "#Convert y true and false to 1 and 0\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "\n",
    "#Drop last column from dataframe\n",
    "df = df.drop(df.columns[[-1]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "\n",
    "iris_mlp = MLP(4,7,3)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 1000\n",
    "c = iris_mlp.train(epochs, 0.8)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "iris_mlp.human_readable_feedforward(xs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris MLP with Cross Entropy\n",
    "iris_mlp = MLP_CrossEntropy(4,7,3)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "c = iris_mlp.train(epochs, 0.19)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "#Initial impressions\n",
    "#Used 1.0 learning constant same for mean squarred error\n",
    "# Cross Entropy was 1 second faster than MSE\n",
    "# Converged faster than MSE\n",
    "# Diverages easier than MSE\n",
    "# MSE is more stable than Cross Entropy\n",
    "# Needs a lower learning constant than MSE\n",
    "\n",
    "#A learning constant of > 0.2 causes divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breast Cancer Wisconsin (Diagnostic) Data Set\n",
    "\n",
    "#Import bc_data.csv into a dataframe\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', header=None)\n",
    "\n",
    "#1st column is id drop it\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Drop 1st row from dataframe\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "#1st column is diagnosis use it as output\n",
    "training_outputs = df.iloc[:,0]\n",
    "\n",
    "#Convert M and B to 1 and 0\n",
    "training_outputs = np.where(training_outputs == 'M', 1, 0)\n",
    "\n",
    "#Convert training_outputs to numpy array\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "#Drop 1st column from dataframe\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "bc_mlp = MLP(30,22,1)\n",
    "\n",
    "bc_mlp.train_inputs = training_inputs\n",
    "bc_mlp.train_outputs = training_outputs\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "c = bc_mlp.train(epochs, 0.04)\n",
    "\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "\n",
    "#Initially did 30 neurons in hidden layer and 1 output neuron\n",
    "#Highest accuracy was 0.70 and 0.22\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 Iris Dataset  using generic MLP\n",
    "#Import iris_data.csv into a dataframe\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Last column is the output\n",
    "training_outputs = df.iloc[:,-1]\n",
    "\n",
    "training_outputs = pd.get_dummies(training_outputs)\n",
    "\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "\n",
    "#Convert y true and false to 1 and 0\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "\n",
    "#Drop last column from dataframe\n",
    "df = df.drop(df.columns[[-1]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "\n",
    "input_size = 4\n",
    "hidden_layer_sizes = [4,4]\n",
    "output_size = 3\n",
    "\n",
    "iris_mlp = generic_MLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "\n",
    "epochs = 6000\n",
    "\n",
    "c = iris_mlp.train(epochs, 0.39)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "#iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "#Problem 4 Iris Dataset  using generic MLP Cross Entropy\n",
    "\n",
    "print('Iris Dataset using generic MLP Cross Entropy')\n",
    "\n",
    "input_size = 4\n",
    "hidden_layer_sizes = [4,4]\n",
    "output_size = 3\n",
    "\n",
    "ciris_mlp = generic_MLP_CrossEntropy(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "ciris_mlp.train_inputs = training_inputs\n",
    "ciris_mlp.train_outputs = training_outputs\n",
    "\n",
    "c = ciris_mlp.train(epochs, 0.1)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "#ciris_mlp.human_readable_feedforward(xs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breast Cancer Wisconsin (Diagnostic) Data Set using generic MLP\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', header=None)\n",
    "\n",
    "#1st column is id drop it\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Drop 1st row from dataframe\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "#1st column is diagnosis use it as output\n",
    "training_outputs = df.iloc[:,0]\n",
    "\n",
    "#Convert M and B to 1 and 0\n",
    "training_outputs = np.where(training_outputs == 'M', 1, 0)\n",
    "\n",
    "#Convert training_outputs to numpy array\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "#Drop 1st column from dataframe\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "\n",
    "\n",
    "input_size = 30\n",
    "hidden_layer_sizes = [25,5]\n",
    "output_size = 1\n",
    "\n",
    "bc_mlp = generic_MLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "bc_mlp.train_inputs = training_inputs\n",
    "bc_mlp.train_outputs = training_outputs\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 6000\n",
    "\n",
    "c = bc_mlp.train(epochs, 0.028)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "print('Cross Entropy')\n",
    "bc_mlp = generic_MLP_CrossEntropy(input_size, hidden_layer_sizes, output_size)\n",
    "bc_mlp.train_inputs = training_inputs\n",
    "bc_mlp.train_outputs = training_outputs\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "c = bc_mlp.train(epochs, 0.028)\n",
    "print(\"cost: \" + str(c[-1]))\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 10 percent of breast cancer data and use it as test data\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', header=None)\n",
    "\n",
    "#1st column is id drop it\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#Drop 1st row from dataframe\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "\n",
    "#Convert M and B to 1 and 0 in 1st column\n",
    "df.iloc[:,0] = np.where(df.iloc[:,0] == 'M', 1, 0)\n",
    "\n",
    "total_rows = df.shape[0]\n",
    "rows_to_select = int(0.1 * total_rows)\n",
    "random_rows = df.sample(n=rows_to_select, random_state=42)\n",
    "\n",
    "random_rows = random_rows.reset_index(drop=True)\n",
    "\n",
    "#Get output column\n",
    "test_outputs = random_rows.iloc[:,0]\n",
    "\n",
    "#Get input columns\n",
    "test_inputs = random_rows.iloc[:,1:]\n",
    "\n",
    "#Convert to numpy array\n",
    "test_outputs = test_outputs.to_numpy()\n",
    "test_inputs = test_inputs.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "test_outputs = test_outputs.astype(int)\n",
    "test_inputs = test_inputs.astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Breast Cancer Wisconsin (Diagnostic) Data Set using validation MLP\n",
    "\n",
    "df = pd.read_csv('bc_data.csv', header=None)\n",
    "\n",
    "#1st column is id drop it\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Drop 1st row from dataframe\n",
    "df = df.drop(df.index[0])\n",
    "\n",
    "#1st column is diagnosis use it as output\n",
    "training_outputs = df.iloc[:,0]\n",
    "\n",
    "#Convert M and B to 1 and 0\n",
    "training_outputs = np.where(training_outputs == 'M', 1, 0)\n",
    "\n",
    "#Convert training_outputs to numpy array\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "#Drop 1st column from dataframe\n",
    "df = df.drop(df.columns[[0]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "\n",
    "input_size = 30\n",
    "hidden_layer_sizes = [22]\n",
    "output_size = 1\n",
    "\n",
    "bc_mlp = validation_MLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "bc_mlp.train_inputs = training_inputs\n",
    "bc_mlp.train_outputs = training_outputs\n",
    "\n",
    "bc_mlp.validation_inputs = test_inputs\n",
    "bc_mlp.validation_outputs = test_outputs\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "epochs = 3000\n",
    "\n",
    "c, validation_c = bc_mlp.train(epochs, 0.04)\n",
    "\n",
    "bc_mlp.human_readable_feedforward(xs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation data iris dataset\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Get random 10 percent of data\n",
    "total_rows = df.shape[0]\n",
    "rows_to_select = int(0.1 * total_rows)\n",
    "random_rows = df.sample(n=rows_to_select, random_state=42)\n",
    "\n",
    "random_rows = random_rows.reset_index(drop=True)\n",
    "\n",
    "#Dummy encode output column\n",
    "test_outputs = random_rows.iloc[:,-1]\n",
    "\n",
    "test_outputs = pd.get_dummies(test_outputs)\n",
    "\n",
    "test_outputs = test_outputs.to_numpy()\n",
    "\n",
    "#Convert y true and false to 1 and 0\n",
    "test_outputs = test_outputs.astype(int)\n",
    "\n",
    "#Drop last column from dataframe\n",
    "random_rows = random_rows.drop(random_rows.columns[[-1]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "\n",
    "test_inputs = random_rows.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "test_inputs = test_inputs.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 4 Iris Dataset  using validation MLP\n",
    "#Import iris_data.csv into a dataframe\n",
    "df = pd.read_csv('iris_data.csv', header=None)\n",
    "\n",
    "#Last column is the output\n",
    "training_outputs = df.iloc[:,-1]\n",
    "\n",
    "training_outputs = pd.get_dummies(training_outputs)\n",
    "\n",
    "training_outputs = training_outputs.to_numpy()\n",
    "\n",
    "#Convert y true and false to 1 and 0\n",
    "training_outputs = training_outputs.astype(int)\n",
    "\n",
    "\n",
    "#Drop last column from dataframe\n",
    "df = df.drop(df.columns[[-1]], axis=1)\n",
    "\n",
    "#Convert dataframe to numpy array\n",
    "training_inputs = df.to_numpy()\n",
    "\n",
    "#Clean up data\n",
    "training_inputs = training_inputs.astype(float)\n",
    "\n",
    "xs = training_inputs.T\n",
    "\n",
    "\n",
    "input_size = 4\n",
    "hidden_layer_sizes = [7]\n",
    "output_size = 3\n",
    "\n",
    "iris_mlp = validation_MLP(input_size, hidden_layer_sizes, output_size)\n",
    "\n",
    "iris_mlp.train_inputs = training_inputs\n",
    "iris_mlp.train_outputs = training_outputs\n",
    "\n",
    "iris_mlp.validation_inputs = test_inputs\n",
    "iris_mlp.validation_outputs = test_outputs\n",
    "\n",
    "\n",
    "epochs = 1000\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "c = iris_mlp.train(epochs, 0.8)\n",
    "print(\"cost: \" + str(c[-1][-1]))\n",
    "iris_mlp.human_readable_feedforward(xs)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
